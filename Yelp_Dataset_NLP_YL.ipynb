{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Challenge - NLP\n",
    "\n",
    "BitTiger DS501\n",
    "\n",
    "Sep 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_busi_rev_joint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 488769 entries, 0 to 488768\n",
      "Data columns (total 14 columns):\n",
      "business_id     488769 non-null object\n",
      "categories      488769 non-null object\n",
      "city            488769 non-null object\n",
      "name            488769 non-null object\n",
      "review_count    488769 non-null int64\n",
      "avg_stars       488769 non-null float64\n",
      "cool            488769 non-null int64\n",
      "date            488769 non-null object\n",
      "funny           488769 non-null int64\n",
      "review_id       488769 non-null object\n",
      "stars           488769 non-null int64\n",
      "text            488769 non-null object\n",
      "useful          488769 non-null int64\n",
      "user_id         488769 non-null object\n",
      "dtypes: float64(1), int64(5), object(8)\n",
      "memory usage: 52.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your feature variables, here is the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Steakhouses, Restaurants, Cajun/Creole</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>1546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>0</td>\n",
       "      <td>VETXTwMw6qxzOVDlXfe6Tg</td>\n",
       "      <td>5</td>\n",
       "      <td>went for dinner tonight. Amazing my husband ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>ymlnR8UeFvB4FZL56tCZsA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Steakhouses, Restaurants, Cajun/Creole</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>1546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>0</td>\n",
       "      <td>S8-8uZ7fa5YbjnEtaW15ng</td>\n",
       "      <td>5</td>\n",
       "      <td>This was an amazing dinning experience! ORDER ...</td>\n",
       "      <td>0</td>\n",
       "      <td>9pSSL6X6lFpY3FCRLEH3og</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Steakhouses, Restaurants, Cajun/Creole</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>1546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1nK5w0VNfDlnR3bOz13dJQ</td>\n",
       "      <td>5</td>\n",
       "      <td>My husband and I went there for lunch on a Sat...</td>\n",
       "      <td>1</td>\n",
       "      <td>gm8nNoA3uB4In5o_Hxpq3g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Steakhouses, Restaurants, Cajun/Creole</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>1546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>0</td>\n",
       "      <td>N1Z93BthdJ7FT2p5S22jIA</td>\n",
       "      <td>3</td>\n",
       "      <td>Went for a nice anniversary dinner. Researched...</td>\n",
       "      <td>0</td>\n",
       "      <td>CEtidlXNyQzgJSdF1ubPFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Steakhouses, Restaurants, Cajun/Creole</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>1546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>ir-EVhHyWna7KqYWtj660g</td>\n",
       "      <td>5</td>\n",
       "      <td>Hands down the best meal and service I have ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>9_BhDyzJYf2JwTD9TyXJ4g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                              categories       city  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  Steakhouses, Restaurants, Cajun/Creole  Las Vegas   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  Steakhouses, Restaurants, Cajun/Creole  Las Vegas   \n",
       "2  --9e1ONYQuAa-CB_Rrw7Tw  Steakhouses, Restaurants, Cajun/Creole  Las Vegas   \n",
       "3  --9e1ONYQuAa-CB_Rrw7Tw  Steakhouses, Restaurants, Cajun/Creole  Las Vegas   \n",
       "4  --9e1ONYQuAa-CB_Rrw7Tw  Steakhouses, Restaurants, Cajun/Creole  Las Vegas   \n",
       "\n",
       "                   name  review_count  avg_stars  cool        date  funny  \\\n",
       "0  Delmonico Steakhouse          1546        4.0     0  2017-02-14      0   \n",
       "1  Delmonico Steakhouse          1546        4.0     0  2017-12-04      0   \n",
       "2  Delmonico Steakhouse          1546        4.0     0  2016-08-22      1   \n",
       "3  Delmonico Steakhouse          1546        4.0     0  2016-09-13      0   \n",
       "4  Delmonico Steakhouse          1546        4.0     0  2016-08-08      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  VETXTwMw6qxzOVDlXfe6Tg      5   \n",
       "1  S8-8uZ7fa5YbjnEtaW15ng      5   \n",
       "2  1nK5w0VNfDlnR3bOz13dJQ      5   \n",
       "3  N1Z93BthdJ7FT2p5S22jIA      3   \n",
       "4  ir-EVhHyWna7KqYWtj660g      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  went for dinner tonight. Amazing my husband ha...       0   \n",
       "1  This was an amazing dinning experience! ORDER ...       0   \n",
       "2  My husband and I went there for lunch on a Sat...       1   \n",
       "3  Went for a nice anniversary dinner. Researched...       0   \n",
       "4  Hands down the best meal and service I have ev...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  ymlnR8UeFvB4FZL56tCZsA  \n",
       "1  9pSSL6X6lFpY3FCRLEH3og  \n",
       "2  gm8nNoA3uB4In5o_Hxpq3g  \n",
       "3  CEtidlXNyQzgJSdF1ubPFw  \n",
       "4  9_BhDyzJYf2JwTD9TyXJ4g  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10/27/2016-I had my birthday dinner here and it was worth the splurge.  I have to say something about the d√©cor - it was too plain and sterile and I expected a little more character from this establishment.  I would have added some wood paneling or some framed mirrors.  But the food was worth it.  We ordered the New Orleans BBQ Shrimp, Butternut Squash Ravioli, Traditional New Orleans Gumbo, and of course the 16 oz Rib Eye steak.  Our side dishes included the Country Smashed Potato, Delmonico Creamed Spinach, Buttered Fresh Asparagus, and Sauteed Garlic Mushrooms.  All the dishes we ordered were delicious.  Our steak was medium rare and it was sooo good.  The meat was tender with the right amount of fatty goodness.  My favorite sides were the asparagus (cooked just right, not over or under cooked) and the mushrooms.  The creamed spinach was a little too salty for me.  They added a nice touch to my birthday by serving complimentary appetizer and dessert (mango sorbet)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents=df['text'].values\n",
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 488769\n"
     ]
    }
   ],
   "source": [
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "print(documents.isnull().sum(),len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your target variable (any categorical variable that may be meaningful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, I am interested in perfect (5 stars) and imperfect (1-4 stars) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: target, dtype: bool 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: target_2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make a column and take the values, save to a variable named \"target\"\n",
    "df['target_2']=[1 if element > 4 else 0 for element in df['stars']]\n",
    "df['target']=df['stars']>4\n",
    "print(df['target'][0:5],df['target_2'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may want to look at the statistic of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237791 488769\n"
     ]
    }
   ],
   "source": [
    "# To be implemented\n",
    "target=df['target']\n",
    "print(sum(target),len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenny\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents is your X, target is your y\n",
    "# Now split the data to training set and test set\n",
    "documents_train, documents_test, target_train, target_test = train_test_split(documents, target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342138,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\n",
    "type(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vectors = vectorizer.fit_transform(documents_train).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asparagus',\n",
       " 'ass',\n",
       " 'ate',\n",
       " 'atmosphere',\n",
       " 'attention']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342138, 2000) [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.35126663 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]] ['00', '10', '100', '11', '12']\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab of your tfidf\n",
    "print(vectors.shape,vectors[0:5],words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform your test data\n",
    "y_test = target_test\n",
    "X_test = vectorizer.transform(documents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will need these helper methods pretty soon\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  # np.argsort by default sorts values in ascending order\n",
    "\n",
    "def get_bottom_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the lowest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"mouse\", \"rabbit\"]\n",
    "    '''\n",
    "    pass  # To be implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 20 words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 by average tf-idf\n",
      "['food', 'great', 'good', 'place', 'service', 'time', 'just', 'like', 'best', 'vegas', 'amazing', 'really', 'delicious', 'chicken', 'love', 'ordered', 'restaurant', 'definitely', 'order', 'got']\n"
     ]
    }
   ],
   "source": [
    "avg = np.sum(vectors, axis=0) \n",
    "print(\"top 20 by average tf-idf\")\n",
    "print(get_top_values(avg, 20, words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for top 5 reviews similar to search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not giving you 5 Stars today because of the service. It's Monday morning, Memorial Day, there's no line out here, at 9 o'clock in the morning and there are numbers of empty tables in your restaurant. We ordered our food that part of the service was wonderful but then we felt like they rushed us to get the heck off their table. Could I please finish my cup of coffee for a $50 breakfast?\n"
     ]
    }
   ],
   "source": [
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "search_query=documents_test[0]\n",
    "print(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the drawn review(s) to vector(s)\n",
    "from nltk.corpus import stopwords\n",
    "search_query_vectorized = vectorizer.transform([search_query]).toarray() # search_query needs to be a list format! [search_query]\n",
    "search_query_vectorized\n",
    "search_query_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "simi_score_search=cosine_similarity(search_query_vectorized,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09673526 0.05190409 0.11683443 ... 0.033286   0.06492736 0.07845691]]\n"
     ]
    }
   ],
   "source": [
    "print(simi_score_search[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the top 5 silimar reviews\n",
    "n_rev=5\n",
    "returned_reviews=get_top_values(simi_score_search[0],n_rev,documents_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heather was wonderful. Excellent fast and hot food with the best service and smiles for is this morning. We will always be back  for breakfast here.',\n",
       " \"This place is s must for your Morning start.. Today we came on a Monday and it was pleasant not to have to wait a long line..although we would do it any time because it's worth it...\",\n",
       " 'Breakfast- good; staff- friendly; food service-a little slow; atmosphere- relaxed, not too crowded (Monday morning).',\n",
       " \"I would give this place 0 stars if I could... I'm currently fighting off food poisoning from the food I ate this morning.\",\n",
       " 'Had breakfast this morning and as always the food and service is great.  Maggi was attentive and very sweet, Jose cooked our food and it was delicious.  Five stars!']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare \"I'm not giving you 5 Stars today because of the service. It's Monday morning, Memorial Day, there's no line out here, at 9 o'clock in the morning and there are numbers of empty tables in your restaurant. We ordered our food that part of the service was wonderful but then we felt like they rushed us to get the heck off their table. Could I please finish my cup of coffee for a $50 breakfast?\" \n",
      "with \"I'm jumping on the 2 star train. \r\n",
      "\r\n",
      "They're lucky they are even getting 2 stars from me. The 2 stars come the bartender who was nice and the corned beef hash that was delicious. I feel the buffet is a bit over priced. $33 a head for early bird breakfast and mimosas that are supposed to be bottomless. The food was average at best; nothing \"grand\" about this buffet. The mimosas are not brought to the table, you have to actually go to the bar and get your mimosas. We were at our table for about 20 minutes waiting for the bar to open. The hosts kept telling us the bartender was coming soon but we just ended up waiting even more. \r\n",
      "\r\n",
      "No fruits on a buffet line? What's going on here. It may be time for a restaurant rescue. The best thing was the smoked salmon and that wasn't even that great. The salmon had a great smoke taste but was chewy. \r\n",
      "\r\n",
      "Would I return? No. I have had better experiences in other places that do not have the Vegas hype attached to it. This experience further adds to my feelings of Las Vegas being overrated.\"\n",
      "[[0.05190409]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "print('Compare \"%s\" \\nwith \"%s\"'%(search_query, documents_train[0]))\n",
    "print(cosine_similarity(search_query_vectorized.reshape(1, -1), vectors[1].reshape(1, -1)))\n",
    "print(search_query_vectorized.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "simi_score=list()\n",
    "for index in range(len(documents_train[1:])):\n",
    "    #print('\"%s\" compared with \"%s\"'%(documents_train[0], documents_train[index+1]))\n",
    "    #print('cosine similarity:', cosine_similarity(vectors[0].reshape(1, -1),vectors[index+1].reshape(1, -1)))\n",
    "    simi_score.append(cosine_similarity(search_query_vectorized.reshape(1, -1),vectors[index+1].reshape(1, -1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=cosine_similarity(vectors[0].reshape(1, -1),vectors[1].reshape(1, -1))\n",
    "vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-152eeff6eca0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    915\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[1;32m--> 110\u001b[1;33m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[0;32m    111\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m    112\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "test_2=cosine_similarity(vectors[0],vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[1].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.05190409]]), array([[0.11683443]]), array([[0.00579007]]), array([[0.]]), array([[0.01074607]]), array([[0.03475928]]), array([[0.0314713]]), array([[0.0151044]]), array([[0.01550143]]), array([[0.11866998]])] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(simi_score[0:10], type(simi_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([269006,  83141,  83138, ..., 292396, 270285,  65694], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted=np.sort(simi_score,axis=None)\n",
    "sorted\n",
    "sorted_ind=np.argsort(simi_score,axis=None)\n",
    "sorted_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow. Just pretty bad . I was running errands and I stopped in since I was in that area, just because I had a coupon for a nice new salad. Well I pull in heading towards the driveway and there were cones blocking the entrance. Ok. I\\'ll get out and go in to order my avocado salad with the coupon . The girl screams \" we can\\'t take credit cards or coupons our system is down \" Ok . Mientras todos los empleados estan hablando en Espa√±ol tan ruidoso de frente de los clientes que no son hispanohablantes. This can be perceived by non speaking clients as rude, I speak and understand Spanish. This isn\\'t a legit Mexican place , Not a local place that caters to a Latino clientele . I know that being in the service industry myself, corporate at that, it is a No-No. English should be spoken up in the front and Spanish in the back. So anyways, the girl tells me no credit. I nodded. Then I pull out the coupon to show her the new salad I wanted, and before I can finish , she says \" No coupons \" I said \"I heard you the first time\", I am just showing you the salad I wanted. I never received a receipt. Even though she should have hand written one . There was no apology, or offer of a soda or something for their inconvenience. I really like their food, even though it is fast food, but I will never go to this location again. Whoever is the manager should have been on the line where all these issues were happening. I did see a note on the way out on the door that wasn\\'t up when I arrived, \"Cash Only \" in pen writing, but again NO APOLOGY. I understand things go wrong, because I\\'m in hospitality , but seriously ??? Corporate please take note !!!!! It was not right what happened today. Issues happen , but with no apologies or making up for a mistake as such as this is where you dropped the ball.'\n",
      " 'Very impressed with the place. Very fast service - from being seated to have the table fully served for all you can eat - just 10 minutes!!! Plus, they do reservations for 10 of more people in VIP room. Will bring friends next time.'\n",
      " 'I came here for a quick dinner before a show. Service was good. I had their street tacos. I only ate 2 and gave one to my friend. She had empanada and they were really small. The free flan was delicious. However, this place is not good enough to come all the way from your hotel to try it.'\n",
      " 'Dates wrapped with bacon. And lamb chops. \\r\\n\\r\\nHeavenly and I would go there everyday if I could. \\r\\n\\r\\nThe bartender was super nice and polite.'\n",
      " '\"I don\\'t know where to begin\" that\\'s what went through my head when I walked in. The variety of the food is outrageous, the quality outstanding , the presentation and effort they put on the food was mouth watering. I felt like I was walking into the food version of Disney park. I tried a little of everything and wasn\\'t disappointed at all, worth the price. Wasn\\'t sure how much the price per person was but for two adult we paid about 75/80 so average of 35/40 per person. Like I said definitely worth the price!']\n"
     ]
    }
   ],
   "source": [
    "# Let's find top 5 similar reviews\n",
    "\n",
    "n = 5\n",
    "sorted_5=sorted_ind[len(sorted)-6:-1]\n",
    "print(documents_train[sorted_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search query:\n",
      "I'm not giving you 5 Stars today because of the service. It's Monday morning, Memorial Day, there's no line out here, at 9 o'clock in the morning and there are numbers of empty tables in your restaurant. We ordered our food that part of the service was wonderful but then we felt like they rushed us to get the heck off their table. Could I please finish my cup of coffee for a $50 breakfast?\n",
      "Vectorized query: [[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Our search query:')\n",
    "print(search_query)\n",
    "print(\"Vectorized query:\", search_query_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 5 similar reviews:\n",
      "['Wow. Just pretty bad . I was running errands and I stopped in since I was in that area, just because I had a coupon for a nice new salad. Well I pull in heading towards the driveway and there were cones blocking the entrance. Ok. I\\'ll get out and go in to order my avocado salad with the coupon . The girl screams \" we can\\'t take credit cards or coupons our system is down \" Ok . Mientras todos los empleados estan hablando en Espa√±ol tan ruidoso de frente de los clientes que no son hispanohablantes. This can be perceived by non speaking clients as rude, I speak and understand Spanish. This isn\\'t a legit Mexican place , Not a local place that caters to a Latino clientele . I know that being in the service industry myself, corporate at that, it is a No-No. English should be spoken up in the front and Spanish in the back. So anyways, the girl tells me no credit. I nodded. Then I pull out the coupon to show her the new salad I wanted, and before I can finish , she says \" No coupons \" I said \"I heard you the first time\", I am just showing you the salad I wanted. I never received a receipt. Even though she should have hand written one . There was no apology, or offer of a soda or something for their inconvenience. I really like their food, even though it is fast food, but I will never go to this location again. Whoever is the manager should have been on the line where all these issues were happening. I did see a note on the way out on the door that wasn\\'t up when I arrived, \"Cash Only \" in pen writing, but again NO APOLOGY. I understand things go wrong, because I\\'m in hospitality , but seriously ??? Corporate please take note !!!!! It was not right what happened today. Issues happen , but with no apologies or making up for a mistake as such as this is where you dropped the ball.'\n",
      " 'Very impressed with the place. Very fast service - from being seated to have the table fully served for all you can eat - just 10 minutes!!! Plus, they do reservations for 10 of more people in VIP room. Will bring friends next time.'\n",
      " 'I came here for a quick dinner before a show. Service was good. I had their street tacos. I only ate 2 and gave one to my friend. She had empanada and they were really small. The free flan was delicious. However, this place is not good enough to come all the way from your hotel to try it.'\n",
      " 'Dates wrapped with bacon. And lamb chops. \\r\\n\\r\\nHeavenly and I would go there everyday if I could. \\r\\n\\r\\nThe bartender was super nice and polite.'\n",
      " '\"I don\\'t know where to begin\" that\\'s what went through my head when I walked in. The variety of the food is outrageous, the quality outstanding , the presentation and effort they put on the food was mouth watering. I felt like I was walking into the food version of Disney park. I tried a little of everything and wasn\\'t disappointed at all, worth the price. Wasn\\'t sure how much the price per person was but for two adult we paid about 75/80 so average of 35/40 per person. Like I said definitely worth the price!']\n"
     ]
    }
   ],
   "source": [
    "print('Most %s similar reviews:' % n)\n",
    "print(documents_train[sorted_5])  # this give the same results as the output of last cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Does the result make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Yes, the top 5 are all negative reviews like the search query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Naive-Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(vectors, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8075513389334128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model.score(vectors, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074145303516992"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_1= LogisticRegression()\n",
    "model_1.fit(vectors,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8342014041117911"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_1.score(vectors, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8306019873014574"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04143952e+00,  1.06228843e+00,  1.26364661e+00, ...,\n",
       "         1.17322515e+00, -2.89675024e+00,  7.55185028e-04]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'incredible',\n",
       " 'thank',\n",
       " 'awesome',\n",
       " 'phenomenal',\n",
       " 'perfection',\n",
       " 'heaven',\n",
       " 'delicious',\n",
       " 'perfect',\n",
       " 'highly',\n",
       " 'fantastic',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'favorite',\n",
       " 'impeccable',\n",
       " 'outstanding',\n",
       " 'love',\n",
       " 'holy',\n",
       " 'fabulous']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_word = 20\n",
    "get_top_values(model_1.coef_[0], n_word, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    went for dinner tonight. Amazing my husband ha...\n",
      "1    This was an amazing dinning experience! ORDER ...\n",
      "2    My husband and I went there for lunch on a Sat...\n",
      "4    Hands down the best meal and service I have ev...\n",
      "5    ABSOLUTE MUST IN VEGAS! Loved everything my bo...\n",
      "Name: text, dtype: object\n",
      "2000\n",
      "top 30 by positive tf-idf\n",
      "['food', 'great', 'place', 'good', 'service', 'delicious', 'time', 'vegas', 'best', 'like', 'amazing', 'just', 'definitely', 'really', 'friendly', 'try', 'love', 'restaurant', 'nice', 'ordered']\n"
     ]
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n_pos = 20\n",
    "rev_pos=df[df['stars']>=4]['text']\n",
    "print(rev_pos[0:5])\n",
    "pos_vec=vectorizer.transform(rev_pos).toarray()\n",
    "avg_pos = np.sum(pos_vec>0, axis=0) \n",
    "print(len(avg_pos))\n",
    "print(\"top 30 by positive tf-idf\")\n",
    "print(get_top_values(avg_pos,n_pos,words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: delightful,beautifully sounds positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     Went for a nice anniversary dinner. Researched...\n",
      "7     I had high hopes for Delmonico's Steakhouse in...\n",
      "11    Good food.  Horrible service.  Had dinner in e...\n",
      "12    My wife and I were very excited to visit the r...\n",
      "19    Great ribeye steak.  Cooked perfectly.  Was a ...\n",
      "Name: text, dtype: object\n",
      "top 20 by neg tf-idf\n",
      "['french', 'greet', 'kimchi', 'located', 'shame', 'owner', 'plenty', 'owners', 'till', 'difficult', 'downside', 'mouth', 'grits', 'attentive', 'cakes', 'opinion', 'appetizers', 'salty', 'red', 'bed']\n"
     ]
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n_neg= 20\n",
    "rev_neg=df[df['stars']<4]['text']\n",
    "print(rev_neg[0:5])\n",
    "neg_vec=vectorizer.transform(rev_neg).toarray()\n",
    "avg_neg = np.sum(neg_vec, axis=0) \n",
    "print(\"top 20 by neg tf-idf\")\n",
    "print(get_top_values(avg_neg,n_neg,words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: difficult,downside,grits,salty sounds negative, frech,greet,shame,plenty,red,kimchi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Random Forest Classifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier(n_estimators=100,max_depth=20,min_samples_leaf=20, random_state=1)\n",
    "model_2= OneVsRestClassifier(estimator,n_jobs=-1)\n",
    "model_2.fit(vectors,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796812981896194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_2.score(vectors, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901262352435706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What do you see from the training score and the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: the training set has a better score than the test set, suggesting overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Can you tell what features (words) are important by inspecting the RFC model?\n",
    "The results below suggest the words in the food categories have the highest tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 by average tf-idf\n",
      "['hookah', 'ramen', 'pho', 'karaoke', 'udon', 'crawfish', 'donuts', 'bagel', 'boba', 'donut', 'indian', 'mi', 'kbbq', 'gyro', 'filipino', 'catfish', 'smoothie', 'pastrami', 'greek', 'philly']\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "print(\"top 20 by average tf-idf\")\n",
    "print(get_top_values(avg, n, words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #1: Use cross validation to evaluate your classifiers\n",
    "\n",
    "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be implemented\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_mse_arr = np.array([])\n",
    "test_mse_arr = np.array([])\n",
    "\n",
    "#for i in pca_range:\n",
    "\n",
    "    #train_subset = vectors[:, :i]\n",
    "\n",
    "    #model_1.fit(train_subset, target_train)\n",
    "\n",
    "    # Get train error\n",
    "train_mse = cross_val_score(model_1, vectors, y=target_train,\n",
    "                                scoring='neg_mean_squared_error', cv=5) * -1\n",
    "train_mse_arr = np.append(train_mse_arr, train_mse.mean())\n",
    "    \n",
    "    # Get test error\n",
    "test_set = X_test\n",
    "test_mse = mean_squared_error(model_1.predict(test_set), y_test)\n",
    "test_mse_arr = np.append(test_mse_arr, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16763606 0.16858596 0.16770912 0.1702081  0.17110455] [0.16939801]\n"
     ]
    }
   ],
   "source": [
    "print(train_mse,test_mse_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #2: Use grid search to find best predictable classifier\n",
    "\n",
    "\n",
    "[sklearn grid search tutorial (with cross validation)](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "[sklearn grid search documentation (with cross validation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Choose the type of classifier. \n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "param_grid = {'n_estimators': [50], \n",
    "              'max_features': ['auto'], \n",
    "              'criterion': ['gini'],\n",
    "              'max_depth': [10,20], \n",
    "              'min_samples_split': [2],\n",
    "              'min_samples_leaf': [10,20],\n",
    "              'n_jobs':[-1]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Run the grid search\n",
    "# read theory\n",
    "grid_obj = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(vectors, target_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "clf.fit(vectors, target_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
